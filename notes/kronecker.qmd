---
title: "Kronecker products"
date: today
---

\newcommand{\C}{\mathbf C}
\newcommand{\G}{\mathbf G}
\newcommand{\Z}{\mathbf Z}
\newcommand{\X}{\mathbf X}
\newcommand{\b}{\mathbf b}
\newcommand{\bbeta}{\boldsymbol \beta}
\newcommand{\btheta}{\boldsymbol \theta}
\newcommand{\inv}[1]{{#1}^{-1}}

We would ideally like to be able to fit models with the covariance matrices constructed from a *generic* Kronecker product, e.g. `ar1(time)` $\otimes$ `cs(treatment)`


* AS-REML syntax: `(f1|g1):(f2|g2)`
* for intercept-only construct $\Z$ matrix via (something like) `fac2sparse(interaction(f1, f2))` - **how do we do it for non-scalar REs?** (`interaction` + `KhatriRao` in some order ...
* once we have the $\Z$ matrix defined, the conditional log-likelihood of observations ($\log {\cal L}(y_i|\bbeta, \b)$) takes care of itself via  $\X \bbeta + \Z \b$.

How about $\log {\cal L}(\b|\btheta)$ ?

* construct precision ($\inv{\G}$) matrix by getting the precision matrices $\inv{\G_1}$, $\inv{\G_2}$; then $\inv{G} = \inv{\G_1} \otimes \inv{\G_2}$ (see [Emi's cheatsheet](https://github.com/emitanaka/cheatsheets/blob/master/linear-algebra.qmd)) or [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product). **Need to make sure the ordering of $\inv G$ matches the ordering of the `b` vector**, which means checking the ordering of the results of  `interaction(levels(f1), levels(f2))` vs. the ordering of `kron(invG1, invG2)` (`invG` matrix rows/cols should correspond to levels of `f`)
* `blockNumTheta` (i.e., number of $\theta$ parameters required for the term) will be the sum of `blockNumTheta` for each component (we do need to keep track of the individual values); ditto for `blockSize` (length of the `b` vector)
* Once we have $\inv{\G_1}$ we should be able to compute $\log {\cal L}(\b|\btheta)$ for each block in the same way as we currently do for all different types in `termwise_nll`. Not quite sure how we would work with precision matrices/specify a MVN log-likelihood with precision rather than corr Cholesky factor supplied??

This suggests that we might want to refactor `termwise_nll` so that we have an upstream function that returns the *precision matrix* given {`theta`, `blockSize`}. (Can this be applied completely generally or are there special cases? Right now we use the state-space representation for `ar1()` (but *not* for Toeplitz?), but I'm not convinced that's necessary (since AR1 has a closed-form precision matrix: machinery in `nlme`, plus can look it up elsewhere)?)

New data required: need to store the codes (`blockCode` and `blockNumTheta`) for each of the factors. To allow for multiple Kronecker products $\G_1 \otimes \G_2 \otimes \G_3 \otimes \ldots$, this can be a vector. I think we could generalize the `per_term_info` struct so that, for a Kronecker product of `n` factors,

- `blockCode` is a variable-length vector (length `n+1`) of integers ; if the *first* element matches the `kron` enum value, then the subsequent elements are the block codes for the factors. (If it is length-1 and not `kron`, then it is treated as a single code.)
- `blockNumTheta` is of length `n` - number of `theta` values for each factor

The `blockSize` component is a scalar (sum of block sizes for individual components; do we need to be able to have this information available elsewhere, e.g. if we want to reconstruct the individual factors?), as is `blockReps`. The `dist` (for spatial cov structs) and `times` (for time cov structs) are a little more problematic; what if there is more than one? Do we want (somehow) to generalize this component to "extra information required for this term"? Should there be a (possibly empty) vector and a (possibly empty) matrix associated with each block, whose meanings and use may vary depending on the cov type?

**Or**: is there some kind of object-oriented solution to this (i.e. make a new structure that somehow inherits from the simpler version)?

Will also need to implement some new covariance structures (`cor` versions of existing structures), or map variance to 1, for identifiability. Mapping will work but could get very complicated; as a design principle I would like to avoid requiring mapping *internally*, leaving any mapping for special cases needed by the user. Naming convention? `hom`+X = homogeneous-diagonal version, `cor`+X = correlation version? (e.g. `homus` is an unstructured correlation matrix + homogeneous variance, `corus` is an unstructured correlation matrix (variance fixed to 1). Is there a way to avoid repeating too much code?

## next step

I wanted to illustrate that this works by creating an initial model with `doFit = FALSE`, hacking `Z` etc., but I don't think this will actually work: we have to reconstruct $\inv{G}$ *at every iteration step*, which means the code needs to be inside `src/glmmTMB.cpp` ...

---

## lme4 hacking

We can also do this in `lme4`, by hacking. We'd use the modular machinery (`?lme4::modular`).

* call `lFormula` with a random effects term `(rows:cols|block)` (again, not sure how to do this with non-scalar REs?); this will set up a dense `Lambdat` matrix and an appropriate `Zt` matrix
* make the deviance function
* set up a wrapper function that maps $\btheta'$ (our Kronecker-product/restricted parameter set) to $\btheta$ (the full Cholesky factor that `lmer` is expecting)
* do we have any nice Kronecker-product identities that help us get $\textrm{Chol}\left((\C_1 \C_1^\top) \otimes (\C_2 \C_2^\top)\right)$ the easy way (i.e. not by `chol(kron(crossprod()))`?

Sch√§cke 2004 ([here](https://www.math.uwaterloo.ca/~hwolkowi/henry/reports/kronthesisschaecke04.pdf)) says

$$
A \otimes B = (L_A L_A^\top) \otimes (L_B L_B^\top) = 
(L_A \otimes L_B) (L_A \otimes L_B)^\top
$$

Lazy test

```{r kron1}
set.seed(102)
rc <- function(n) {
    m <- matrix(0, n, n)
    m[lower.tri(m, diag = TRUE)] <- rnorm(n*(n+1)/2)
    m
}
tchol <- function(A) t(chol(A))
A <- rc(3)
B <- rc(3)
c1 <- tchol(kronecker(tcrossprod(A), tcrossprod(B)))
c2 <- kronecker(A,B)
stopifnot(all.equal(c1, c2))
```

    
