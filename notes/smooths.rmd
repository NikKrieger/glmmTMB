---
title: implementing mgcv-style smooths in glmmTMB
---


```{r reinstall_flag}
## make this depend on external input/commandArg/env variable?
reinstall_pkg <- FALSE
```

As previously commented on [the glmmTMB issues list](https://github.com/glmmTMB/glmmTMB/issues/704) and [illustrated by Devin Johnson](https://gist.github.com/dsjohnson/9d66aa47557ad56438aaf75dd25910ea), `mgcv` provides facilities for exporting its smooths in a format that can work with an IID ('ridge') penalty as implemented in standard mixed-model random effects terms.

This is *briefly* described in `?mgcv::smooth2random`, and the appendix of Wood (2004) gives the mathematical details (see below for reference).

**fixme**: pick an example with some nonlinearity for the non-nullspace component of the smooth to capture ...

## current status/example: "sort of" working

I've implemented the 'front end' for this: pick the `s()` terms out of a formula, convert them with `smooth2random`, add those as appropriate to the fixed and random effect model matrices with the RE type set to "homdiag" (iid errors).

For the standard `sleepstudy` example, we get results that are somewhat reasonable/match `mgcv` in some ways. We **must** use REML in order for this to work! (With default ML fit, we get a sort-of-OK fit, but some weird stuff happens - spline parameters go to tiny values, `NA/NaN` warnings, convergence failure ...

```{r first_try}
## this is slightly rude, but should be quick if already installed ...
if (reinstall_pkg) remotes::install_github("glmmTMB/glmmTMB/glmmTMB@mgcv_smooths")
library(glmmTMB)
library(mgcv)
data("sleepstudy", package = "lme4")
gtmb1 <- glmmTMB(formula = Reaction ~ s(Days) + (1 | Subject), data = sleepstudy, REML = TRUE)
mgcv1 <- gam(formula = Reaction ~ s(Days) + s(Subject, bs="re"), data = sleepstudy)
```

Lots of comparisons work: residual std error, intercept parameter and 'fixed' spline parameter (which determines the part of the spline corresponding to the nullspace), the random effects corresponding to the `(1|Subject)`/`s(Subject, bs = "re")` terms ...

```{r compare}
all.equal(sigma(mgcv1), sigma(gtmb1), tol = 1e-5)
## intercept and nullspace fixed effect
all.equal(unname(fixef(gtmb1)$cond),
          unname(coef(mgcv1)[c("(Intercept)", "s(Days).9")]),
          tol = 1e-6)
## fitted values (**not** worrying yet about prediction with newdata)
all.equal(fitted(gtmb1), unname(fitted(mgcv1)), tol = 2e-5)

## random effects
bvec <- gtmb1$fit$parfull
bvec <- bvec[names(bvec) == "b"]
r1 <- ranef(gtmb1)$cond  ## all b-values for the smooth are tiny ... ??
r2 <- split(coef(mgcv1), substr(names(coef(mgcv1)), 1, 3))[c(3,2)]
names(r2) <- names(r1)
all.equal(unname(unlist(r1[[1]])), unname(r2[[1]]), tol = 1e-5)
```

The log-likelihoods do *not* match (something to do with which parameters are profiled out in REML??)

```{r}
c(mgcv = logLik(mgcv1), gtmb1 = logLik(gtmb1))
```

I'm not sure how to extract the values corresponding to RE variances from `mgcv` ...

Works (technically), but annoyingly non-pos-def because the smooth gets penalized to zero ... change to a better example!

```{r}
gtmb0 <- update(gtmb1, . ~  . - (1|Subject))
```

```{r nileex}
data("Nile")
ndat <- data.frame(time = time(Nile), val = c(Nile))
sm0 <- gam(val ~ s(time), data = ndat, method = "REML")
sfun <- function(tstart) {
    glmmTMB(val ~ s(time), data = ndat, REML = TRUE,
            start = list(theta = tstart))
}
tvec <- seq(-2, 5, length = 31)
```

```{r nilefit, cache = TRUE}
smlist <- lapply(tvec, sfun)
```

Note that this works 'reliably' only for sufficiently large starting values of theta ...  is this because
we are fitting random effects on a raw scale rather than relative to SD?  Can we use `log(sd(response_var))` as a 
starting value for theta in these cases? 

```{r nile2}
res <- t(thval <- sapply(smlist, \(x) c(getME(x, "theta"), x$sdr$pdHess)))
plot(tvec, res[,1], pch = 16, col = res[,2]+1)
```

```{r}
sm1 <- glmmTMB(val ~ s(time), data = ndat, REML = TRUE, start = list(theta = 5))
names(sm1$modelInfo$reTrms$cond$smooth_info[[1]])

predict(sm1, newdata = ndat)
```

```{r}
testthat::expect_equal(c(unname(predict(sm0))), predict(sm1))
```

## issues 

There is a lot left to do.

- test test test! See where stuff breaks.
- *predictions*: for *new values of the predictor variables*, some fancy footwork is needed to convert the coefficients back to a form that will allow predictions (see Wood's refs). `smooth2random()` provides a matrix and vector that jointly allow the back-calculation (this is loosely analogous to stuff with `predvars` in base R, I think, making sure that the basis is constructed in a way that's consistent with the original model): we should be able to use
	* in `predict`, use these to back-calculate and modify the coefficients *in place* (restoring at the end of the function as in the "fast prediction" code branch)
	* I think this needs to be done 
	* will need to figure out *which* fixed and random effects correspond to which smooth terms ... compute up-front and carry it along?
	* utility function to get the modified coeffs? Which versions, if any, should be printed in summaries etc.?
	* how does `mgcv` handle predictions with new data? What is the analog of `predvars` so that we can use the previously specified knots etc. ??? (`mgcv:::predict.gam` is 500+ lines long ...) (I think we can use `mgcv::PredictMat` on the stored smooth object ... ?)
* better print/summary handling for smooth terms (remove them from fixed effects/print separately); more options to `fixef()` etc ...
* documentation: where are the pitfalls?
* prettier `VarCorr` printing ... 
* can we set things up so that existing `mgcv` machinery (such as `gratia`) can work on our fits?

## architectural stuff

* at present, `mkReTrms` (which is imported from `lme4`) takes only the bar-terms into account. The current strategy is to take the results and do a bunch of post-processing to add stuff from the smooth terms.  This is clunky.
     * We could modify mkReTrms significantly (time to take it over into `reformulas` and refactor it while keeping it lme4-compliant?) to allow smooth terms to be included *in situ*
     * We could modify the s()-terms to be dummy terms with the right dimensions, then modify in place (this is not much better than the current approach)
* can we get rid of `no_specials` ... ??? 
* right now we're hacking `s()` to `homdiag()`. Should we duplicate the `homdiag` code, or otherwise make "homdiag" and "s" synonyms under the hood?
* this re-opens the can of worms about whether we should allow random effects for the dispersion term (we need to if we're going to allow smooths in the dispersion term ...)

From Devin Wood's post, for reference:

```r
s2rPred <- function(sm,re,data) {
## Function to aid prediction from smooths represented as type==2
## random effects. re must be the result of smooth2random(sm,...,type=2).
  X <- PredictMat(sm,data)   ## get prediction matrix for new data
  ## transform to r.e. parameterization
  if (!is.null(re$trans.U)) X <- X%*%re$trans.U
  X <- t(t(X)*re$trans.D)
  ## re-order columns according to random effect re-ordering...
  X[,re$rind] <- X[,re$pen.ind!=0] 
  ## re-order penalization index in same way  
  pen.ind <- re$pen.ind; pen.ind[re$rind] <- pen.ind[pen.ind>0]
  ## start return object...
  r <- list(rand=list(),Xf=X[,which(re$pen.ind==0),drop=FALSE])
  for (i in 1:length(re$rand)) { ## loop over random effect matrices
    r$rand[[i]] <- X[,which(pen.ind==i),drop=FALSE]
    attr(r$rand[[i]],"s.label") <- attr(re$rand[[i]],"s.label")
  }
  names(r$rand) <- names(re$rand)
  r
} ## s2r
```

## ref

Simon N Wood (2004) Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models, Journal of the American Statistical Association, 99:467, 673-686, DOI: 10.1198/016214504000000980 https://doi.org/10.1198/016214504000000980

